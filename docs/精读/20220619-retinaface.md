## 一、文档更新记录
### 1. 版本信息
- 版本号：v1
- 论文名称：RetinaFace: Single-stage Dense Face Localisation in the Wild [CVPR 201905]
- 创建日期：2022.06.19
- 创建人：扶云

## 二、算法介绍
### 1. 核心思想


1. 在WIDER FACE数据集上`手动标注了5个面部标志`，并观察到在这个额外的监督信号的帮助下，hard face检测结果显著改进;
2. 我们进一步`添加了一个自监督网格解码器分支`，与现有的监督分支并行预测像素级三维形状人脸信息；【一个辅助任务带来检测的精度提升，类似mask rcnn的mask预测】
3. 在WIDER FACE硬测试集上，RetinaFace比目前水平的平均精度(AP)高出1.1%(实现AP等于91.4%)；
4. 通过采用轻量级骨干网，RetinaFace可以在`单个CPU核上实时`运行vga分辨率的图像

> 引入了一个更广泛的人脸定位的定义，包括人类检测、人脸对齐、像素级人脸解析和3D密集对应回归【其实就是引入了多任务】
> 
> 人脸检测跟普通目标检测的区别：人脸检测具有较小的比例变化(从1:1到1:1.5)，但更大的尺度变化(从几个像素到上千像素)。


![20220702094927](https://lcv1-1256975222.cos.ap-shanghai.myqcloud.com/20220702094927.png)

> MTCNN[66]和STN[5]同时检测人脸和5个面部标志。由于训练数据的限制，JDA[6]、MTCNN[66]和STN[5]还没有验证微小人脸检测是否可以受益于5个面部标志的额外监督；
> 在MASK R-CNN[20]中，通过在现有的边界框识别和回归分支的基础上增加用于预测对象mask的分支，显著提高了检测性能。这证实了密集的像素级注释也有利于改进检测。

对于wider face的挑战性人脸检测任务，不可能进行密集的人脸标注(无论是以更多地标的形式还是以语义分段的形式)。由于监督信号不容易获得，问题是我们是否可以应用非监督方法来进一步改进人脸检测：

- 我们使用自监督学习的网格解码器[70]分支与现有的监督分支并行地预测像素级的3D人脸形状。

总结来说：

1. 单阶段设计：采用多任务学习策略来同时预测人脸分数、人脸box、五个人脸标志点【有监督】以及每个人脸像素的3D位置和对应关系【自监督】。
2. 在更广泛的hard face子集上，RetinaFace比最先进的两阶段方法(ISRN[67])的AP性能高1.1%(AP等于91.4%)。【？？？】
3. 在IJB-C数据集上，RetinaFace有助于提高ArcFace的[11]验证精度(当Far[^1]=1e-6时，tar等于89.59%)。这表明，更好的人脸定位可以显著提高人脸识别能力。


### 2. 结论和展望

无


### 3. 实验
#### 数据集介绍
`WIDER FACE`略，[参见](/精读/20220608-SCRFD-样本和计算重分配/#_3)

将人脸数据集分为训练(40%)、验证(10%)和测试(50%)子集。

1. 新增5点landmark: 总共，我们在`训练集上标注了84.6k个人脸，在验证集中标注了18.5k个人脸`
   ![20220702105007](https://lcv1-1256975222.cos.ap-shanghai.myqcloud.com/20220702105007.png)
   不同的人脸质量占比情况如下：
   ![20220702105149](https://lcv1-1256975222.cos.ap-shanghai.myqcloud.com/20220702105149.png)
   

`AFLW2000-3D dataset`：用于验证密集人脸landmark的精度

#### 指标

指标：AP即AP0.5，在iou=0.5的条件下计算的平均精度；mAP即AP(0.5:0.05:0.95)，在IoU=0.5:0.05:0.95分别计算AP之后取平均。

#### 消融实验
下表可以说明：

- 增加5个关键点很有效；
- 仅增加dense 像素回归并没有帮助
- 同时增加以上两个，能进一步提高检测性能。
  
![20220702113327](https://lcv1-1256975222.cos.ap-shanghai.myqcloud.com/20220702113327.png)

#### 精度对比

> 这个图表里面详细的对比了跟其他算法的精度情况，均取得了最好的精度。

![20220702114305](https://lcv1-1256975222.cos.ap-shanghai.myqcloud.com/20220702114305.png)


#### 密集人脸精度

对比了68个landmark和 all landmark下的精度情况，RetinaFace可以轻松处理姿态变化的人脸【(⊙o⊙)…】，但难以预测复杂场景下的精确密集3D估计。

![20220702115433](https://lcv1-1256975222.cos.ap-shanghai.myqcloud.com/20220702115433.png)

#### 人脸识别精度对比

在公平的比较下，TAR(在FAR=1e−6时)只需用RetinaFace取代MTCNN，就能从88.29%显著提高到89.59%。

![20220702115752](https://lcv1-1256975222.cos.ap-shanghai.myqcloud.com/20220702115752.png)

#### 推理耗时

- RetinaFaceResNet-152专为高精度的人脸定位而设计，对于VGA图像(640×480)，运行速度为13 FPS。
- 相比之下，RetinaFace-MobileNet-0.25专为高效的人脸定位而设计，对于4K图像(4096×2160)在GPU上的实时速度为40 FPS，对于高清图像(1920×1080)在多线程CPU上为20 FPS，对于VGA图像(640×480)在单线程CPU上为60 FPS。

![20220702120015](https://lcv1-1256975222.cos.ap-shanghai.myqcloud.com/20220702120015.png)

【略：跟MTCNN比较5个关键点的精度；

### 4. 详细过程

先看下网络结构图：

![20220702105310](https://lcv1-1256975222.cos.ap-shanghai.myqcloud.com/20220702105310.png)

!!! summary "主要模块"
    === "特色金字塔"
        RetinaFace使用从P2到P6的特征金字塔级别，其中P2到P5是从对应的ResNet residual stage(C2到C5)的输出使用自顶向下和横向连接来计算的；

        P6在C5上通过3×3卷积计算，步长=2。

        C1到C5是ImageNet-11k数据集上的预训练ResNet-152[21]分类网络，而P6是用“Xille”方法随机初始化的[17]。

    === "上下文模块"
        受SSH[36]和金字塔[49]的启发，我们还在五个特征金字塔层次上应用了独立的上下文模块，以增加感受野并增强rigid上下文建模能力。
        
        还用可变形卷积网络(DCN)[9，74]取代了侧向连接和上下文模块内的所有3×3卷积层，进一步增强了non-rigid上下文建模能力。
        
    === "loss head"
        对于负锚，仅应用分类损失。对于正锚，计算多任务损失。我们在不同的特征map $H_n×W_n×256，n∈\{2...6\}$。

        对于网格解码器，我们应用了预先训练的模型[70]，高效推理、计算开销小。

    === "anchor 设置"
        长宽比是1:1；当输入图像大小为640×640时，锚点可以覆盖特征金字塔层次上从16×16到406×406的尺度，总共有102,300个anchors，其中75%来自P2。
        ![20220702112544](https://lcv1-1256975222.cos.ap-shanghai.myqcloud.com/20220702112544.png)

        - 在训练过程中，`当IOU大于0.5时，anchor被匹配到gt box，当IOU小于0.3时，anchor被匹配到背景`。不匹配的anchor在训练期间被忽略。
        - 由于大多数锚(>99%)在匹配步骤后是negative，我们使用标准的OHEM[47，68]来缓解正训练样本和负训练样本之间的显著失衡。更具体地说，我们根据损失值对负锚进行排序，并选择前几个锚，`使负样本与正样本的比例至少为3：1`。

    === "数据增强"
        由于在更广泛的人脸训练集中大约有20%的小人脸，我们遵循[68，49]，从原始图像中随机裁剪正方形小块，并将这些小块调整为640×640以生成更大的训练人脸。从原始图像中`以原始图像短边的[0.3，1]之间的随机大小裁剪`正方形面片。


我们在四个NVIDIA Tesla P40(24 GB)GPU上使用SGD优化器(动量为0.9%，权重衰减为0.0005，批大小为8×4)训练RetinaFace。

学习速率从10−3开始，在5个周期后上升到10−2，然后在55和68个周期除以10。训练过程在80个epochs结束。

## 三、附件
[原始论文](https://arxiv.org/abs/1905.00641)

[github](https://github.com/deepinsight/insightface/tree/master/RetinaFace)

[^1]: 
    跟TPR/FPR不同，这里的TAR（True Accept Rate）和FAR（False Accept Rate）主要是应用在人脸识别中，$FAR=\frac{非同人分数 > T}{非同人比较的次数}$，也就是我们比较不同人的图像时，把其中的图像对当成同一个人图像的比例。我们希望FAR越小越好。TAR类比。